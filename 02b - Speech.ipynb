{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech\n",
    "\n",
    "また、人工知能(AI)システムと会話をすることで、多くの場合、音声による応答を期待してコミュニケーションをとることができるようになると期待されています。\n",
    "\n",
    "<p style='text-align:center'><img src='./images/speech.jpg' alt='話しているロボット'/></p> </p>\n",
    "\n",
    "*音声認識* (話し言葉を解釈するAIシステム）と*音声合成*（話し言葉の応答を生成するAIシステム）は、音声対応AIソリューションの主要なコンポーネントです。\n",
    "\n",
    "\n",
    "## Cognitive Servicesリソースの作成\n",
    "\n",
    "これらのレビューのテキストを分析するには、**Text Analytics**コグニティブサービスを使用できます。これを使用するには、Azureサブスクリプションで**Text Analytics**または**Cognitive Services**リソースのいずれかをプロビジョニングする必要があります(これが唯一のサービスであるか、またはその使用状況を個別に追跡したい場合は、Text Analyticsリソースを使用してください。それ以外の場合は、コグニティブサービスリソースを使用してText Analyticsサービスを他のコグニティブサービスと組み合わせることができます。)\n",
    "\n",
    "まだ持っていない場合は、次の手順を使用して、Azureサブスクリプションで**Cognitive Services**リソースを作成します。\n",
    "\n",
    "1. 別のブラウザタブで、https://portal.azure.com の Azure ポータルを開き、Microsoft アカウントでサインインします。\n",
    "2. **[&#65291;リソースの作成]**ボタンをクリックして、*Cognitive Services*を検索し、次の設定で**Cognitive Services**リソースを作成します。:\n",
    "    - **Name**:  *一意の名前を入力してください*\n",
    "    - **Subscription**: *あなたのAzureサブスクリプション*\n",
    "    - **Location**: *利用可能な場所*\n",
    "    - **Pricing tier**: S0\n",
    "    - **Resource group**: *固有の名前を持つリソースグループ*を作成\n",
    "3. デプロイが完了するのを待ちます。次に、コグニティブサービスリソースに移動し、**クイックスタート**ページで、キーとエンドポイントに注意してください。クライアント アプリケーションからコグニティブ サービス リソースに接続するには、これらが必要です。\n",
    "\n",
    "## Cognitive Servicesリソースのキーとエンドポイントを取得する\n",
    "\n",
    "認知サービス リソースを使用するには、クライアント アプリケーションにそのエンドポイントと認証キーが必要です。\n",
    "\n",
    "1. Azure ポータルで、コグニティブサービスリソースの **クイックスタート** ページで、リソースの **Key1** をコピーして、**YOUR_COG_KEY** を置き換えて、以下のコードに貼り付けます。\n",
    "2. リソースの **endpoint** をコピーして、以下のコードに貼り付けて、 **YOUR_COG_ENDPOINT** を置き換えます。\n",
    "3. 以下のセルを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cog_key = 'YOUR_COG_KEY'\n",
    "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
    "cog_region = 'YOUR_COG_REGION'\n",
    "\n",
    "print('Ready to use cognitive services in {} using key {}'.format(cog_region, cog_key))"
   ]
  },
  {
   "source": [
    "これでCognitive Servicesのセットアップが完了しました。\n",
    "\n",
    "そしてPythonから本演習を実行するには、まずAzureの関連パッケージをインストールする必要があります。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure.cognitiveservices.speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 音声認識\n",
    "\n",
    "例えば、\"ライトをつけて \"や \"ライトを消して \"といった音声による指示を受け付けるホームオートメーションシステムを構築したいとします。アプリケーションは、音声ベースの入力（音声による指示）を受け取り、それをテキストに書き写すことで解釈し、それを解析して分析できるようにする必要があります。\n",
    "\n",
    "これで、音声を書き写す準備ができました。入力はマイクでも音声ファイルでも構いません。\n",
    "\n",
    "では、音声を書き写すために Speech サービスの Speech to Text 機能を使ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
    "\n",
    "# 音声ファイルから音声コマンドを取得する\n",
    "file_name = 'light-on.wav'\n",
    "audio_file = os.path.join('data', 'speech', file_name)\n",
    "\n",
    "# 音声認識装置の設定\n",
    "speech_config = SpeechConfig(cog_key, cog_region)\n",
    "audio_config = AudioConfig(filename=audio_file) # デフォルト(マイク)の代わりにファイルを使う\n",
    "speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n",
    "\n",
    "# スピーチを書き写すために一回限りの同期呼び出しを使用します\n",
    "speech = speech_recognizer.recognize_once()\n",
    "\n",
    "# オーディオを再生し、書き写したテキストを表示する\n",
    "IPython.display.display(IPython.display.Audio(audio_file, autoplay=True),\n",
    "                        IPython.display.HTML(speech.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数 **file_name** を *light-off.wav* に変更して、セルを再度実行してみてください。サービスは両方のファイルを正しくテキストに書き写すことができるはずです。\n",
    "\n",
    "## 音声合成\n",
    "\n",
    "ここまでで、スピーチサービスを使って音声をテキストに書き写す方法を見てきましたが、逆の場合はどうでしょうか？テキストを音声に変換するにはどうすればいいのでしょうか？\n",
    "\n",
    "例えば、ホームオートメーションシステムがライトを点けろというコマンドを解釈したとしましょう。適切な反応は、口頭でコマンドを認識することかもしれません（実際にコマンドされたタスクを実行するのと同様に！）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
    "\n",
    "# テキストを話してもらう\n",
    "response_text = 'Turning the light on.'\n",
    "\n",
    "# 音声合成を設定します\n",
    "speech_config = SpeechConfig(cog_key, cog_region)\n",
    "output_file = os.path.join('data', 'speech', 'response.wav')\n",
    "audio_output = AudioConfig(filename=output_file) # デフォルト(スピーカー)の代わりにファイルを使う\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config, audio_output)\n",
    "\n",
    "# テキストを音声に書き写す\n",
    "result = speech_synthesizer.speak_text(response_text)\n",
    "\n",
    "# 出力されたオーディオファイルを再生する\n",
    "IPython.display.display(IPython.display.Audio(output_file, autoplay=True),\n",
    "                        IPython.display.Image(data=os.path.join(\"data\", \"speech\" , response_text.lower() + 'jpg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数 **response_text** を *Turning the light off.* (最後のピリオドを含む) に変更して、セルをもう一度実行して結果を聞いてみてください。\n",
    "\n",
    "## Learn more\n",
    "\n",
    "このノートでは、Speech Cognitve Servicesを使用した非常に簡単な例を見ました。Speech サービスのドキュメントでは、[speech-to-text](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-speech-to-text) と [text-to-speech](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-text-to-speech) について詳しく説明しています。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}