{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 光学式文字認識(OCR)\n",
    "\n",
    "<p style='text-align:center'><img src='./images/ocr.jpg' alt='新聞を読むロボット'/></p>\n",
    "\n",
    "一般的なコンピュータビジョンの課題は、画像中のテキストを検出して解釈することです。この種の処理は、しばしば*光学式文字認識* (OCR) と呼ばれています。\n",
    "\n",
    "## コンピュータビジョンサービスを使って画像中のテキストを読み取る\n",
    "\n",
    "**Computer Vision**コグニティブサービスは、以下のようなOCRタスクをサポートします。\n",
    "\n",
    "- 複数の言語のテキストを読み取るために使用できる **OCR** API。この API は同期的に使用でき、画像内の少量のテキストを検出して読み取る必要がある場合に有効です。\n",
    "- 大きな文書に最適化された **Read** API。このAPIは非同期で使用され、印刷されたテキストと手書きのテキストの両方に使用することができます。\n",
    "\n",
    "このサービスは、**Computer Vision**リソースまたは**Cognitive Services**リソースを作成することで使用できます。\n",
    "\n",
    "まだ作成していない場合は、Azure サブスクリプションで **Cognitive Services** リソースを作成してください。\n",
    "\n",
    "1. 別のブラウザタブで、https://portal.azure.com の Azure ポータルを開き、Microsoft アカウントでサインインします。\n",
    "2. **[&#65291;リソースの作成]**ボタンをクリックして、*Cognitive Services* を検索し、次の設定で**Cognitive Services**リソースを作成します。:\n",
    "    - **Name**: *一意の名前を入力してください*.\n",
    "    - **Subscription**: *Azureサブスクリプション*.\n",
    "    - **Location**: *利用可能なリージョンを選択します*.\n",
    "    - **Pricing tier**: S0\n",
    "    - **Resource group**: *一意な名前を持つリソースグループを作成します*.\n",
    "3. デプロイが完了するのを待ちます。次に、Cognitive Servicesリソースに移動し、**クイックスタート** ページで、キーとエンドポイントに注意してください。クライアント アプリケーションからCognitive Services リソースに接続するには、これらが必要です。\n",
    "\n",
    "### Cognitive Servicesのキーとエンドポイント用の変数を作成します。\n",
    "\n",
    "Cognitive Servicesリソースに接続するためには、キーとエンドポイントが必要です。\n",
    "\n",
    "1. Azureポータルで、Cognitive Servicesリソースの**クイックスタート**ページを表示します。\n",
    "2. リソースの**Key1**をコピーして、**YOUR_COG_KEY**を置き換えて、以下のコードに貼り付けます。\n",
    "3. リソースの **endpoint** をコピーして、**YOUR_COG_ENDPOINT**.を置き換えて、以下のコードに貼り付けます。\n",
    "4. 下のセルの緑色の<span style=\"color:green\">&#9655;</span>ボタンをクリックして、コードを実行します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cog_key = 'YOUR_COG_KEY'\n",
    "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
    "\n",
    "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
   ]
  },
  {
   "source": [
    "これでCognitive Servicesのセットアップが完了しました。\n",
    "\n",
    "そしてPythonから本演習を実行するには、まずAzureの関連パッケージをインストールする必要があります。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-cognitiveservices-vision-computervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、コンピュータビジョンサービスを使用して画像内のテキストを読み取る準備が整いました。\n",
    "\n",
    "まず、**OCR** API を使用して、画像を同期的に解析し、画像に含まれるテキストを読み取ることができます。この例では、Northwind Tradersという架空の小売会社の広告画像があり、その中にテキストが含まれています。それを読み取るために以下のセルを実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# computer visionサービスのクライアントを獲得する\n",
    "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
    "\n",
    "# 画像ファイルを読み込む\n",
    "image_path = os.path.join('data', 'ocr', 'advert.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Computer Visionサービスを利用して，画像中のテキストを検索します．\n",
    "read_results = computervision_client.recognize_printed_text_in_stream(image_stream)\n",
    "\n",
    "# テキストを一行ずつ処理する\n",
    "for region in read_results.regions:\n",
    "    for line in region.lines:\n",
    "\n",
    "        # テキストの行の中の単語を読む\n",
    "        line_text = ''\n",
    "        for word in line.words:\n",
    "            line_text += word.text + ' '\n",
    "        print(line_text.rstrip())\n",
    "\n",
    "# 画像を開いて表示します。\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "img = Image.open(image_path)\n",
    "draw = ImageDraw.Draw(img)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像内で見つかったテキストは、領域、行、単語の階層構造に整理されており、コードはこれらを読み込んで結果を取得します。\n",
    "\n",
    "結果では、画像の上に読み込んだテキストを表示します。\n",
    "\n",
    "## バウンディングボックスを表示する\n",
    "\n",
    "結果には、画像内で見つかったテキストの行と個々の単語の*バウンディングボックス*座標も含まれています。以下のセルを実行して、上記で取得した広告画像のテキスト行のバウンディングボックスを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# 画像ファイルを読み込む\n",
    "image_path = os.path.join('data', 'ocr', 'letter.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# コンピュータビジョンサービスのクライアントを獲得する\n",
    "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
    "\n",
    "# 画像内の印刷された文字を読み込んで操作IDを取得するリクエストを送信する\n",
    "read_operation = computervision_client.read_in_stream(image_stream,\n",
    "                                                      raw=True)\n",
    "operation_location = read_operation.headers[\"Operation-Location\"]\n",
    "operation_id = operation_location.split(\"/\")[-1]\n",
    "\n",
    "# 非同期操作が完了するのを待ちます\n",
    "while True:\n",
    "    read_results = computervision_client.get_read_result(operation_id)\n",
    "    if read_results.status not in [OperationStatusCodes.running]:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# 操作が成功した場合は、テキストを1行ずつ処理します。\n",
    "if read_results.status == OperationStatusCodes.succeeded:\n",
    "    for result in read_results.analyze_result.read_results:\n",
    "        for line in result.lines:\n",
    "            print(line.text)\n",
    "\n",
    "# 画像を開いて表示します。\n",
    "print('\\n')\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "img = Image.open(image_path)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果として、テキストの各イネのバウンディングボックスが画像上に矩形で表示されます。\n",
    "\n",
    "## Read APIを使用する\n",
    "\n",
    "以前に使用した OCR API は、少量のテキストを含む画像でも問題なく動作します。スキャンしたドキュメントなど、より大きなテキストを読み取る必要がある場合は、**Read** APIを使用できます。このためには、複数のステップのプロセスが必要です。\n",
    "\n",
    "1. コンピュータビジョンサービスに画像を送信して、非同期で読み取りと分析を行います。\n",
    "2. 解析操作が完了するのを待ちます。\n",
    "3. 解析結果を取得します。\n",
    "\n",
    "次のセルを実行して、このプロセスを使用して、Northwind Tradersの店長にスキャンした手紙のテキストを読み取る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import TextOperationStatusCodes, TextRecognitionMode\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果を確認してください。手書きの署名と印刷されたテキストのほとんどが構成されている手紙の完全な転写があります。手紙の元の画像は、OCRの結果の下に表示されています（見るにはスクロールする必要があるかもしれません）。\n",
    "\n",
    "## 手書きのテキストを読む\n",
    "\n",
    "先の例では、画像を解析するリクエストで、*印刷された*テキストに対して動作を最適化するテキスト認識モードが指定されていました。にもかかわらず、手書きの署名が読み取られたことに注意してください。しかし、主に*手書き*の文書を分析する必要がある場合は、テキスト認識モードを手書きに最適化するように設定することができます。\n",
    "\n",
    "たとえば、買い物リストを含むノートを書き、携帯電話のアプリを使用してノートを読み取り、その中に含まれるテキストを書き写すとします。\n",
    "\n",
    "以下のセルを実行して、手書きの買い物リストの読み取り操作の例を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# 画像ファイルを読み込む\n",
    "image_path = os.path.join('data', 'ocr', 'note.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# コンピュータビジョンサービスのクライアントを獲得する\n",
    "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
    "\n",
    "# 画像内の手書き文字を読みたいというリクエストを出して、操作IDを取得する\n",
    "read_operation = computervision_client.read_in_stream(image_stream,\n",
    "                                                      raw=True)\n",
    "operation_location = read_operation.headers[\"Operation-Location\"]\n",
    "operation_id = operation_location.split(\"/\")[-1]\n",
    "\n",
    "# 非同期操作が完了するのを待ちます\n",
    "while True:\n",
    "    read_results = computervision_client.get_read_result(operation_id)\n",
    "    if read_results.status not in [OperationStatusCodes.running]:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# 操作が成功した場合は、テキストを1行ずつ処理します。\n",
    "if read_results.status == OperationStatusCodes.succeeded:\n",
    "    for result in read_results.analyze_result.read_results:\n",
    "        for line in result.lines:\n",
    "            print(line.text)\n",
    "\n",
    "# 画像を開いて表示します。\n",
    "print('\\n')\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "img = Image.open(image_path)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Information\n",
    "\n",
    "OCR のためのコンピュータビジョンサービスの使用については、[コンピュータビジョンのドキュメント]（https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-recognizing-text）を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}