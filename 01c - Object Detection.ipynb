{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# オブジェクト検出\n",
    "\n",
    "*オブジェクト検出*は、コンピュータビジョンの一形態で、機械学習モデルを訓練して、画像内のオブジェクトの個々のインスタンスを分類し、その位置を示す*境界線の箱*を示すようにします。これは、*画像分類*（モデルが「これは何の画像か」という質問に答える）から、モデルに「この画像の中にどのようなオブジェクトがあり、どこにあるか」と尋ねることができるソリューションを構築することへの進行と考えることができます。\n",
    "\n",
    "<p style='text-align:center'><img src='./images/object-detection.jpg' alt='A robot identifying fruit'/></p>\n",
    "\n",
    "例えば、食料品店がオブジェクト検出モデルを使用して、カメラを使用してコンベアベルトをスキャンし、ベルト上に各アイテムを置いて個別にスキャンしなくても特定のアイテムを識別できる自動レジシステムを実装する場合があります。\n",
    "\n",
    "Microsoft Azureの**Custom Vision**コグニティブサービスは、カスタムオブジェクト検出モデルを作成して公開するためのクラウドベースのソリューションを提供します。\n",
    "\n",
    "\n",
    "## Custom Vision リソースの作成\n",
    "\n",
    "カスタムビジョンサービスを使用するには、モデルのトレーニングに使用できるAzureリソースと、アプリケーションが使用できるように公開できるリソースが必要です。これらのタスクのそれぞれに同じリソースを使用することもできますし、同じリージョンに作成された両方のリソースを使用して別々にコストを配分することもできます。どちらか（または両方）のタスクのリソースは、一般的な **Cognitive Services** リソース、または特定の **Custom Vision** リソースにすることができます。新しい **Custom Vision** リソースを作成するには、次の手順を使用します（既存のリソースがある場合は、既存のリソースを使用することもできます）。\n",
    "\n",
    "1. 新しいブラウザタブで、[https://portal.azure.com](https://portal.azure.com)のAzureポータルを開き、Azureサブスクリプションに関連付けられたMicrosoftアカウントを使用してサインインします。\n",
    "2. **&#65291;リソースの作成**ボタンを選択し、**Custom Vision**を検索し、以下の設定で**カスタムビジョン**リソースを作成します。\n",
    "    - **Create options**: Both\n",
    "    - **Subscription**: *Your Azure subscription*\n",
    "    - **Resource group**: *Create a new resource group with a unique name*\n",
    "    - **Name**: *Enter a unique name*\n",
    "    - **Training location**: *Choose any available region*\n",
    "    - **Training pricing tier**: F0\n",
    "    - **Prediction location**: *The same as the training location*\n",
    "    - **Prediction pricing tier**: F0\n",
    "\n",
    "    > **Note**: すでにF0 Custom Visionを使用している場合は、**S0**を選択してください。\n",
    "\n",
    "3. プロジェクトが作成され、ブラウザで開くのを待ちます。\n",
    "\n",
    "## Custom Vision プロジェクトの作成\n",
    "\n",
    "オブジェクト検出モデルをトレーニングするには、トレーニングリソースに基づいてカスタムビジョンプロジェクトを作成する必要があります。これを行うには、Custom Visionポータルを使用します。\n",
    "\n",
    "1. 新しいブラウザ タブで [https://customvision.ai](https://customvision.ai) のカスタム ビジョン ポータルを開き、Azure サブスクリプションに関連付けられた Microsoft アカウントを使用してサインインします。\n",
    "2. 次の設定で新規プロジェクトを作成します:\n",
    "    - **Name**: Grocery Detection\n",
    "    - **Description**: Object detection for groceries.\n",
    "    - **Resource**: *The Custom Vision resource you created previously*\n",
    "    - **Project Types**: Object Detection\n",
    "    - **Domains**: General\n",
    "3. プロジェクトが作成され、ブラウザで開くのを待ちます。\n",
    "\n",
    "\n",
    "## 画像の追加とタグ付け\n",
    "\n",
    "オブジェクト検出モデルを学習するには、モデルが識別したいクラスを含む画像をアップロードし、各オブジェクトインスタンスのバウンディングボックスを示すタグを付ける必要があります。\n",
    "\n",
    "\n",
    "1. https://aka.ms/fruit-objects からトレーニング画像をダウンロードして抽出します。抽出したフォルダには、果物の画像集が入っています。\n",
    "2. 2. カスタムビジョンポータルのオブジェクト検出プロジェクトで、 **画像の追加** を選択し、抽出したフォルダ内のすべての画像をアップロードします。\n",
    "3. 3. 画像がアップロードされたら、最初の画像を選択して開きます。\n",
    "4. 4. 下の画像のように自動的に検出された領域が表示されるまで、画像内の任意のオブジェクトの上にマウスを置きます。その後、オブジェクトを選択し、必要に応じて、それを囲むように領域のサイズを変更します。\n",
    "\n",
    "    <p style='text-align:center'><img src='./images/object-region.jpg' alt='The default region for an object'/></p>\n",
    "\n",
    "　あるいは、オブジェクトの周りをドラッグして領域を作成することもできます。\n",
    "\n",
    "\n",
    "5. 領域がオブジェクトを囲んでいる場合は、ここに示すように、適切なオブジェクトタイプ(*apple*, *banana*, または *orange*)の新しいタグを追加します。\n",
    "\n",
    "\n",
    "    <p style='text-align:center'><img src='/images/object-tag.jpg' alt='画像内のタグ付きオブジェクト'/></p>\n",
    "\n",
    "6. 画像内の他のオブジェクトを選択してタグを付け、必要に応じて領域のサイズを変更し、新しいタグを追加します。\n",
    "\n",
    "     <p style='text-align:center'><img src='/images/object-tags.jpg' alt='画像内の2つのタグ付きオブジェクト'/></p>\n",
    "\n",
    "7. 右側の**>**リンクを使って次の画像に移動し、そのオブジェクトにタグを付けます。その後、リンゴ、バナナ、オレンジをそれぞれタグ付けしながら、画像コレクション全体を作業してください。\n",
    "\n",
    "\n",
    "8. 最後の画像のタグ付けが終わったら、**画像の詳細**エディタを閉じ、**トレーニング画像**ページの**タグ**で、**タグ**を選択して、タグ付けされたすべての画像を表示します。\n",
    "\n",
    "    <p style='text-align:center'><img src='/images/tagged-images.jpg' alt='プロジェクト内のタグ付き画像'/></p>\n",
    "\n",
    "## モデルのトレーニングとテスト\n",
    "\n",
    "プロジェクト内の画像にタグを付けたら、モデルをトレーニングする準備ができました。\n",
    "\n",
    "1. カスタムビジョンプロジェクトで、**トレーニング**をクリックして、タグ付けされた画像を使用してオブジェクト検出モデルをトレーニングします。**クイックトレーニング**オプションを選択します。\n",
    "2. 2. トレーニングが完了するのを待ち（10分ほどかかる場合があります）、*Precision*、*Recall*、および*mAP*のパフォーマンスメトリクスを確認します（これらは分類モデルの予測精度を測定するもので、すべてが高いはずです）。\n",
    "3. ページの右上で、**クイックテスト**をクリックし、**画像URL**ボックスに「https://aka.ms/apple-orange」と入力して、生成された予測を表示します。その後、**クイックテスト**ウィンドウを閉じます。\n",
    "\n",
    "\n",
    "\n",
    "## オブジェクト検出モデルを公開し，消費します．\n",
    "\n",
    "これで、訓練されたモデルを公開し、クライアントアプリケーションから使用する準備が整いました。\n",
    "\n",
    "1. **パフォーマンス**ページの左上にある **&#128504; Publish**をクリックして、以下の設定で訓練済みモデルをパブリッシュします。\n",
    "    - **モデル名**: detect-produce\n",
    "    - **予測リソース**。*あなたの認知サービスリソース\n",
    "\n",
    "2. 公開後、**Performance**ページの右上にある*settings* (&#9881;)アイコンをクリックして、プロジェクト設定を表示します。次に、**一般**（左側）の下にある**プロジェクトID**をコピーして、**YOUR_PROJECT_ID**の代わりに下のコードセルに貼り付けてください。\n",
    "\n",
    "> (*この演習の最初に**Custom Vision**リソースを作成する代わりに**Cognitive Services**リソースを使用した場合、プロジェクト設定の右側からそのキーとエンドポイントをコピーし、下のコードセルに貼り付けて、結果を見るために実行することができます。それ以外の場合は、以下の手順を実行して、カスタムビジョン予測リソースのキーとエンドポイントを取得します*)。\n",
    "\n",
    "3. **Project Settings**ページの左上にある*Project Gallery* (&#128065;) アイコンをクリックして、Custom Vision ポータルのホームページに戻ります。\n",
    "\n",
    "4. Custom Vision ポータルのホームページの右上にある *settings* (&#9881;) アイコンをクリックして、Custom Vision サービスの設定を表示します。次に、**Resources**で、*prediction*リソース(<u>トレーニングリソースではありません)を展開し、**Key**と**Endpoint**の値を以下のコードセルにコピーし、**YOUR_KEY**と**YOUR_ENDPOINT**を置き換えます。\n",
    "    \n",
    "5. 下のコードセルを実行し、緑色の <span style=\"color:green\">&#9655</span> ボタン（セルの左上）をクリックして、変数をプロジェクト ID、キー、エンドポイントの値に設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'YOUR_PROJECT_ID' # ご自身のProject IDに置き換えてください\n",
    "cv_key = 'YOUR_KEY' # ご自身の予測キーに置き換えてください\n",
    "cv_endpoint = 'YOUR_ENDPOINT' # ご自身の予測エンドポイントに置き換えてください\n",
    "\n",
    "model_name = 'detect-produce' # あなたがモデルをPublishした時入力したモデル名にマッチする必要があります(大文字小文字含めて)\n",
    "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
   ]
  },
  {
   "source": [
    "クライアントアプリケーションは、上記の詳細を使用して、カスタムビジョンのオブジェクト検出モデルに接続することができます。\n",
    "\n",
    "そしてPythonからCustom Visionサービスを使うには、まずAzure Cognitive Services Custom Visionのパッケージをインストールする必要があります。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install azure-cognitiveservices-vision-customvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "これであなたのキーとエンドポイントでCustom Visionを使う準備が完了します。\n",
    "\n",
    "以下のコードセルを実行して、モデルを使用して画像内の個々の農産物アイテムを検出します。\n",
    "\n",
    "> **注意**。コードの詳細についてはあまり気にしないでください。これは、Python SDK for the Custom Vision サービスを使用して、画像をモデルに送信し、検出されたオブジェクトの予測値を取得します。各予測は、クラス名（*apple*, *banana*, *orange*）と、予測されたオブジェクトが画像のどこで検出されたかを示す*bounding box*座標で構成されています。コードは，この情報を利用して，画像上の各オブジェクトの周りにラベル付きの箱を描きます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# テスト画像をロードしてサイズを取得します\n",
    "test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\n",
    "test_img = Image.open(test_img_file)\n",
    "test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\n",
    "\n",
    "# オブジェクト検出モデルの予測クライアントを取得します\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\n",
    "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\n",
    "\n",
    "print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\n",
    "\n",
    "# テスト画像内のオブジェクトを検出します.\n",
    "with open(test_img_file, mode=\"rb\") as test_data:\n",
    "    results = predictor.detect_image(project_id, model_name, test_data)\n",
    "\n",
    "# 結果を表示するための図を作成する\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.axis('off')\n",
    "\n",
    "# 検出された各物体を囲んだボックスで画像を表示します．\n",
    "draw = ImageDraw.Draw(test_img)\n",
    "lineWidth = int(np.array(test_img).shape[1]/100)\n",
    "object_colors = {\n",
    "    \"apple\": \"lightgreen\",\n",
    "    \"banana\": \"yellow\",\n",
    "    \"orange\": \"orange\"\n",
    "}\n",
    "for prediction in results.predictions:\n",
    "    color = 'white' # default for 'other' object tags\n",
    "    if (prediction.probability*100) > 50:\n",
    "        if prediction.tag_name in object_colors:\n",
    "            color = object_colors[prediction.tag_name]\n",
    "        left = prediction.bounding_box.left * test_img_w \n",
    "        top = prediction.bounding_box.top * test_img_h \n",
    "        height = prediction.bounding_box.height * test_img_h\n",
    "        width =  prediction.bounding_box.width * test_img_w\n",
    "        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\n",
    "        draw.line(points, fill=color, width=lineWidth)\n",
    "        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検出されたオブジェクトと各予測の確率を示す結果の予測を表示します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}