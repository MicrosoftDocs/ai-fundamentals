{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Understanding\n",
    "\n",
    "コンピュータがAIを使って、話し言葉や入力されたコマンドを自然言語で理解できるようになることがますます期待されています。例えば、「電気をつけて」「扇風機をつけて」などの音声コマンドを使って自宅の機器を制御し、AIを搭載した機器にコマンドを理解させて適切なアクションを取らせるホームオートメーションシステムを実装したいとします。\n",
    "\n",
    "<p style='text-align:center'><img src='./images/language_understanding.jpg' alt='聞いているロボット'/>\n",
    "\n",
    "\n",
    "## オーサリングと予測のリソースを作成する\n",
    "\n",
    "Microsoft のCognitive Servicesには、Language Understanding Intelligence Service（LUIS）が含まれており、これを使用すると、*発話*に基づいて*エンティティ*に適用される*意図*を定義できます。**Language Understanding**または**Cognitive Services**リソースのいずれかを使用してLUISアプリを*公開*することができますが、アプリをオーサリングするためには、別個の**Language Understanding**リソースを作成する必要があります。\n",
    "\n",
    "1. 別のブラウザタブで、[https://portal.azure.com](https://portal.azure.com)のAzureポータルを開き、Microsoftアカウントでサインインします。\n",
    "2.  **+リソースの作成**をクリックし、*Language Understanding*を検索します。\n",
    "3. サービスの一覧で、**Language Understanding**をクリックします。\n",
    "4.  **Language Understanding**ブレードで、**Create**をクリックします。\n",
    "5. [**作成**] ブレードで、次の詳細を入力し、[**作成**] をクリックします。\n",
    "  - **Create option**: 両方\n",
    "  - **Name**: *ユニークな名前*\n",
    "  - **Subscription**: *Azureのサブスクリプション*を選択してください。\n",
    "  - **Resource Group**: *既存のリソースグループを選択するか、新しいリソースグループを作成します。\n",
    "  - **Authoring location**: *利用可能な場所を選択してください。\n",
    "  - **Authoring pricing tier**: F0\n",
    "  - **Runtime location**: *オーサリングの場所*と同じ\n",
    "  - **Runtime pricing tier**: F0\n",
    "6. リソースが作成されるのを待ち、オーサリング用と予測用の 2 つの言語理解リソースがプロビジョニングされていることに注意してください。これらのリソースは、作成したリソースグループに移動して表示することができます。\n",
    "\n",
    "### LUISアプリを作成する\n",
    "\n",
    "LUISで自然言語理解を実装するには、アプリを作成し、アプリに理解させたいコマンドを定義するためにエンティティ、インテント、発言を追加します。\n",
    "    \n",
    "1. 新しいブラウザタブで、[https://preview.luis.ai](https://preview.luis.ai) のLUISポータルを開き、Azureサブスクリプションに関連付けられたMicrosoftアカウントを使用してサインインします。初めてLUISにサインインする場合は、アプリにアカウント詳細にアクセスするための権限を与える必要があるかもしれません。次に、Azureサブスクリプションで作成した既存のLUISオーサリングリソースを選択して、*Welcome*ステップを完了します。\n",
    "    \n",
    "2. ＊＊My Apps**ページを開き、サブスクリプションとLUISオーサリングリソースを選択します。次に、次の設定で会話用の新しいアプリを作成します。\n",
    "  - **Name**: Home Automation\n",
    "  - **Culture**: Japanese\n",
    "  - **Description**: Simple home automation\n",
    "  - **Prediction resource**: *あなたのLUIS予測リソース*\n",
    "3. 効果的なLUISアプリを作成するためのヒントが表示されたら、パネルを閉じます。\n",
    "    \n",
    "    \n",
    "### エンティティを作成する\n",
    "\n",
    "エンティティとは、言語モデルが識別して何かを行うことができるものです。このケースでは、LUISアプリはオフィスの照明や扇風機などの様々な*デバイス*を制御するために使用されます。各デバイスタイプごとに、デバイスの名前（例：*ライト*）と、このタイプのデバイスを参照するために使用される可能性のある同義語（例：*ランプ*）を識別するサブリストを作成します。\n",
    "\n",
    "1. アプリのLUISページで、左側のペインで**エンティティ**をクリックします。次に、**Create**をクリックし、**device**という名前の新規エンティティを作成し、**List**タイプを選択して、**Next**をクリックします。\n",
    "2. **リストエンティティの作成**ページで、*Add new sublist*をクリックし、**ライト**と入力してENTERを押します。\n",
    "3. After the **light** entity has been added, under **Synonyms**, click *Type in value*, and type **lamp** and press ENTER.\n",
    "3. **ライト**エンティティが追加されたら、**Synonyms**の下の*Type in value*をクリックし、**ランプ**と入力してENTERを押します。\n",
    "4.  **ファン**という名前の2つ目のサブリストを作成し、**換気扇**を追加します。\n",
    "5. **Create**をクリックして、**ライト**と**ファン**のサブリストを含む**device**エンティティを作成します。\n",
    "\n",
    "### インテントの作成\n",
    "\n",
    "*意図(intent)*とは、1つまたは複数のエンティティに対して実行したいアクションのことで、例えば、ライトをオンにしたい、ファンをオフにしたいなどです。この場合、2 つのインテントを定義します。1 つはデバイスをオンにするためのインテント、もう1つはデバイスをオフにするためのインテントです。それぞれのインテントに対して、そのインテントを示すために使用される言語の種類を示すサンプル *発話* を指定します。\n",
    "\n",
    "    \n",
    "1. 左側のペインで、**Intents** をクリックします。次に、**Create** をクリックし、**switch_on** という名前のインテントを追加します。\n",
    "2. **switch_on**インテントの**Utterances**ページで、**ライトを点けて**と入力して**Enter**を押し、この発言をリストに追加します。\n",
    "3. 発話のリストで、*ライトを点けて* 発話の中の\"ライト\"という単語をクリックして、**device** > **ライト**エンティティに割り当てます。\n",
    "4. 4. **switch_on**インテントに、***ファンをオンにして***というフレーズで、2つ目の発話を追加します。次に、**device** > **ファン** エンティティに「ファン」という単語を割り当てます。\n",
    "5. 5. 左側のペインで、**Intents**をクリックして**Create**をクリックし、**switch_off**という名前の2つ目のインテントを追加します。\n",
    "6. 6. **switch_off**インテントの**Utterances**ページで、**ライトを消して***という語句を追加し、**device** > **ライト**エンティティに「ライト」という語句を割り当てます。\n",
    "7. 7. **switch_off** インテントに、***ファンを止めて***というフレーズで、2 番目の発話を追加します。次に、「ファン」という単語を **device** > **ファン** エンティティに接続します。\n",
    "\n",
    "\n",
    "\n",
    "### 言語モデルのトレーニングとテスト\n",
    "\n",
    "これで、LUISアプリの言語モデルを学習するために、エンティティ、インテント、子宮の形で提供されたデータを使用する準備が整いました。\n",
    "\n",
    "1. アプリのLUISページのトップで、**Train**をクリックして言語モデルを訓練します。\n",
    "2. モデルが学習されたら、**Test**をクリックし、Testペインを使用して、以下のフレーズの予測された意図を表示します。:\n",
    "    * ライトをつける\n",
    "    * ファンの電源を切る\n",
    "    * ランプを消す\n",
    "    * 換気扇のスイッチを入れてください。\n",
    "3. [テスト] ウィンドウを閉じます。\n",
    "    \n",
    "### モデルの公開とエンドポイントの構成\n",
    "\n",
    "学習したモデルをクライアント・アプリケーションで使用するには、クライアント・アプリケーションが新しい発話を送信できるエンドポイントとして公開しなければなりません。\n",
    "    \n",
    "1. アプリのLUISページのトップで、**Publish**をクリックします。次に、**Production slot**を選択し、**Done**をクリックします。\n",
    "2. モデルが公開されたら、アプリのLUISページ上部の**Manage**をクリックします。次に、**アプリ情報**タブで、アプリの**アプリID**をメモします。これをコピーし、以下のコードに貼り付けて、**YOUR_LUIS_APP_ID**を置き換えます。\n",
    "3. **Azure Resources** タブで、コグニティブサービスリソースに基づいている言語モデルの **Primary key** と **Endpoint URL** に注意してください。これらをコピーして、**YOUR_LUIS_KEY**と**YOUR_LUIS_ENDPOINT**を置き換えて、以下のコードに貼り付けます。\n",
    "4. セルの左上にある緑色の<span style=\"color:green\">&#9655;</span>ボタンをクリックして下のセルを実行し、プロンプトが表示されたら（このページの一番上にある）、*turn the light on*というテキストを入力してください。テキストはあなたのLUISモデルによって解釈され、適切な画像が表示されます。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_code import luis\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    # APIの設定\n",
    "    luis_app_id = 'YOUR_LUIS_APP_ID'\n",
    "    luis_key = 'YOUR_LUIS_KEY'\n",
    "    luis_endpoint = 'YOUR_LUIS_ENDPOINT'\n",
    "\n",
    "    # コマンドのプロンプト\n",
    "    command = input('Please enter a command: \\n')\n",
    "\n",
    "    # 予測されたインテントとエンティティを取得する (python_code.home_auto.py内のコード)\n",
    "    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, command)\n",
    "\n",
    "    # 適切な画像を表示する\n",
    "    img_name = action + '.jpg'\n",
    "    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n",
    "    plt.axis('off')\n",
    "    plt. imshow(img)\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のセルを再実行して、次のフレーズを試してみてください。\n",
    "\n",
    "* *ライトをつける*\n",
    "* *ランプを消す*\n",
    "* *ファンのスイッチを入れる*\n",
    "* *ライトをつける*\n",
    "* *ライトを消す*\n",
    "* *ファンの電源を切る*\n",
    "* *換気扇の電源を入れる*\n",
    "\n",
    "> **Note**: LUISアプリからintentとentititesを取得するためのコードに興味がある場合は、**python_code**フォルダ内の**luis.py**ファイルを参照してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ボイスコントロールを追加\n",
    "\n",
    "これまでは、テキストを分析する方法を見てきましたが、AIシステムによって、人間が音声認識を使ってソフトウェアサービスと通信できるようになるケースが増えてきています。これをサポートするために、**Speech**コグニティブサービスは、話し言葉をテキストに書き写す簡単な方法を提供します。\n",
    "\n",
    "## Cognitive Servicesリソースの作成\n",
    "\n",
    "これらのレビューのテキストを分析するには、**Text Analytics**コグニティブサービスを使用できます。これを使用するには、Azureサブスクリプションで**Text Analytics**または**Cognitive Services**リソースのいずれかをプロビジョニングする必要があります(これが唯一のサービスであるか、またはその使用状況を個別に追跡したい場合は、Text Analyticsリソースを使用してください。それ以外の場合は、コグニティブサービスリソースを使用してText Analyticsサービスを他のコグニティブサービスと組み合わせることができます。)\n",
    "\n",
    "まだ持っていない場合は、次の手順を使用して、Azureサブスクリプションで**Cognitive Services**リソースを作成します。\n",
    "\n",
    "1. 別のブラウザタブで、https://portal.azure.com の Azure ポータルを開き、Microsoft アカウントでサインインします。\n",
    "2. **[&#65291;リソースの作成]**ボタンをクリックして、*Cognitive Services*を検索し、次の設定で**Cognitive Services**リソースを作成します。:\n",
    "    - **Name**:  *一意の名前を入力してください*\n",
    "    - **Subscription**: *あなたのAzureサブスクリプション*\n",
    "    - **Location**: *利用可能な場所*\n",
    "    - **Pricing tier**: S0\n",
    "    - **Resource group**: *固有の名前を持つリソースグループ*を作成\n",
    "3. デプロイが完了するのを待ちます。次に、コグニティブサービスリソースに移動し、**クイックスタート**ページで、キーとエンドポイントに注意してください。クライアント アプリケーションからコグニティブ サービス リソースに接続するには、これらが必要です。\n",
    "\n",
    "## Cognitive Servicesリソースのキーとエンドポイントを取得する\n",
    "\n",
    "認知サービス リソースを使用するには、クライアント アプリケーションにそのエンドポイントと認証キーが必要です。\n",
    "\n",
    "1. Azure ポータルで、コグニティブサービスリソースの **クイックスタート** ページで、リソースの **Key1** をコピーして、**YOUR_COG_KEY** を置き換えて、以下のコードに貼り付けます。\n",
    "2. 2. リソースの **endpoint** をコピーして、以下のコードに貼り付けて、 **YOUR_COG_ENDPOINT** を置き換えます。\n",
    "3. 3. 以下のセルを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cog_key = 'YOUR_COG_KEY'\n",
    "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
    "\n",
    "# endpointから地域を取得\n",
    "cog_region = cog_endpoint[8:cog_endpoint.find('.')]\n",
    "\n",
    "print('Ready to use cognitive services in {} using key {}'.format(cog_region, cog_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下のセルを実行して、音声ファイルから音声を書き起こし、LUISアプリのコマンドとして使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_code import luis\n",
    "import os\n",
    "import IPython\n",
    "import os\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
    "\n",
    "try:   \n",
    "\n",
    "    # 音声ファイルから音声コマンドを取得する\n",
    "    file_name = 'light-on.wav'\n",
    "    audio_file = os.path.join('data', 'luis', file_name)\n",
    "\n",
    "    # 音声認識装置の設定\n",
    "    speech_config = SpeechConfig(cog_key, cog_region)\n",
    "    audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n",
    "    speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n",
    "\n",
    "    # スピーチを書き写すために一回限りの同期呼び出しを使用します\n",
    "    speech = speech_recognizer.recognize_once()\n",
    "\n",
    "    # 予測されたインテントとエンティティを取得する (python_code.home_auto.py内のコード)\n",
    "    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, speech.text)\n",
    "\n",
    "    # 適切な画像を取得します\n",
    "    img_name = action + '.jpg'\n",
    "\n",
    "    # オーディオを再生して画像を表示する\n",
    "    IPython.display.display(IPython.display.Audio(audio_file, autoplay=True),\n",
    "                            IPython.display.Image(data=os.path.join(\"data\", \"luis\" ,img_name)))\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のセルを変更して、**light-off.wav**オーディオファイルを使用するようにしてください。\n",
    "\n",
    "## Learn More\n",
    "\n",
    "LUISについては、[サービスドキュメント](https://docs.microsoft.com/azure/cognitive-services/luis/)で詳しく説明しています。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
