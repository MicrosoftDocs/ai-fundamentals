{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "\n",
    "自然言語処理（NLP）は、書き言葉や話し言葉を扱う人工知能（AI）の一分野です。NLPを使用して、テキストや音声から意味的な意味を抽出したり、自然言語で意味のある応答を定式化したりするソリューションを構築することができます。\n",
    "\n",
    "Microsoft Azureの*Cognitive Services*には、*Text Analytics*サービスが含まれており、テキスト内のキーフレーズの識別や、センチメントに基づいたテキストの分類など、すぐに使えるNLP機能を提供しています。\n",
    "\n",
    "<p style='text-align:center'><img src='./images/NLP.jpg' alt='ノートを読むロボット'/></p>\n",
    "\n",
    "例えば、架空の組織*Margie's Travel*が、顧客にホテル宿泊のレビュー投稿を奨励しているとします。テキスト分析サービスを使用して、キーフレーズを抽出してレビューを要約したり、どのレビューが肯定的でどのレビューが否定的かを判断したり、レビューのテキストを分析して、場所や人物などの既知のエンティティについて言及しているかどうかを確認したりすることができます。\n",
    "\n",
    "## レビューのドキュメントを見る\n",
    "\n",
    "まずは、お客様が残してくださったホテルのレビューを見てみましょう。\n",
    "\n",
    "レビューはテキストファイルです。レビューを見るには、下のセルの緑色の<span style=\"color:green\">&#9655;</span>ボタン（セルの左上）をクリックしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# data/reviewsフォルダ内のレビューを読む\n",
    "reviews_folder = os.path.join('data', 'text', 'reviews')\n",
    "\n",
    "# id (ファイル名) と text (内容) のプロパティを持つレビューのコレクションを作成するreviews = []\n",
    "reviews = []\n",
    "for file_name in os.listdir(reviews_folder):\n",
    "    review_text = open(os.path.join(reviews_folder, file_name)).read()\n",
    "    review = {\"id\": file_name, \"text\": review_text}\n",
    "    reviews.append(review)\n",
    "\n",
    "for review_num in range(len(reviews)):\n",
    "    # レビューのテキストを表示します\n",
    "    print('{}\\n{}\\n'.format(reviews[review_num]['id'], reviews[review_num]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cognitive Servicesリソースの作成\n",
    "\n",
    "これらのレビューのテキストを分析するには、**Text Analytics**コグニティブサービスを使用できます。これを使用するには、Azureサブスクリプションで**Text Analytics**または**Cognitive Services**リソースのいずれかをプロビジョニングする必要があります(これが唯一のサービスであるか、またはその使用状況を個別に追跡したい場合は、Text Analyticsリソースを使用してください。それ以外の場合は、コグニティブサービスリソースを使用してText Analyticsサービスを他のコグニティブサービスと組み合わせることができます。)\n",
    "\n",
    "まだ持っていない場合は、次の手順を使用して、Azureサブスクリプションで**Cognitive Services**リソースを作成します。\n",
    "\n",
    "1. 別のブラウザタブで、https://portal.azure.com の Azure ポータルを開き、Microsoft アカウントでサインインします。\n",
    "2. **[&#65291;リソースの作成]**ボタンをクリックして、*Cognitive Services*を検索し、次の設定で**Cognitive Services**リソースを作成します。:\n",
    "    - **Name**:  *一意の名前を入力してください*\n",
    "    - **Subscription**: *あなたのAzureサブスクリプション*\n",
    "    - **Location**: *利用可能な場所*\n",
    "    - **Pricing tier**: S0\n",
    "    - **Resource group**: *固有の名前を持つリソースグループ*を作成\n",
    "3. デプロイが完了するのを待ちます。次に、コグニティブサービスリソースに移動し、**クイックスタート**ページで、キーとエンドポイントに注意してください。クライアント アプリケーションからコグニティブ サービス リソースに接続するには、これらが必要です。\n",
    "\n",
    "## Cognitive Servicesリソースのキーとエンドポイントを取得する\n",
    "\n",
    "認知サービス リソースを使用するには、クライアント アプリケーションにそのエンドポイントと認証キーが必要です。\n",
    "\n",
    "1. Azure ポータルで、コグニティブサービスリソースの **クイックスタート** ページで、リソースの **Key1** をコピーして、**YOUR_COG_KEY** を置き換えて、以下のコードに貼り付けます。\n",
    "2. 2. リソースの **endpoint** をコピーして、以下のコードに貼り付けて、 **YOUR_COG_ENDPOINT** を置き換えます。\n",
    "3. 3. 以下のセルを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cog_key = 'YOUR_COG_KEY'\n",
    "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
    "\n",
    "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
   ]
  },
  {
   "source": [
    "これでCognitive Servicesのセットアップが完了しました。\n",
    "\n",
    "そしてPythonから本演習を実行するには、まずAzureの関連パッケージをインストールする必要があります。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-cognitiveservices-language-textanalytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語の検出\n",
    "まずは、これらのレビューがどのような言葉で書かれているのかを見極めることから始めましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "# Get a client for your text analytics cognitive service resource\n",
    "text_analytics_client = TextAnalyticsClient(endpoint=cog_endpoint,\n",
    "                                            credentials=CognitiveServicesCredentials(cog_key))\n",
    "\n",
    "# Analyze the reviews you read from the /data/reviews folder earlier\n",
    "language_analysis = text_analytics_client.detect_language(documents=reviews)\n",
    "\n",
    "# print detected language details for each review\n",
    "for review_num in range(len(reviews)):\n",
    "    # print the review id\n",
    "    print(reviews[review_num]['id'])\n",
    "\n",
    "    # Get the language details for this review\n",
    "    lang = language_analysis.documents[review_num].detected_languages[0]\n",
    "    print(' - Language: {}\\n - Code: {}\\n - Score: {}\\n'.format(lang.name, lang.iso6391_name, lang.score))\n",
    "\n",
    "    # Add the detected language code to the collection of reviews (so we can do further analysis)\n",
    "    reviews[review_num][\"language\"] = lang.iso6391_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## キーフレーズを抽出する\n",
    "\n",
    "今、あなたは、主要な話のポイントのいくつかの兆候を与えるキーフレーズを識別するために、カスタマーレビューのテキストを分析することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the client and reviews you created in the previous code cell to get key phrases\n",
    "key_phrase_analysis = text_analytics_client.key_phrases(documents=reviews)\n",
    "\n",
    "# print key phrases for each review\n",
    "for review_num in range(len(reviews)):\n",
    "    # print the review id\n",
    "    print(reviews[review_num]['id'])\n",
    "\n",
    "    # Get the key phrases in this review\n",
    "    print('\\nKey Phrases:')\n",
    "    key_phrases = key_phrase_analysis.documents[review_num].key_phrases\n",
    "    # Print each key phrase\n",
    "    for key_phrase in key_phrases:\n",
    "        print('\\t', key_phrase)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "キーフレーズは、各レビューの中で最も重要なトークポイントを理解するのに役立ちます。例えば、\"親切なスタッフ \"や \"サービスが悪い \"というフレーズが含まれているレビューは、レビュアーの主な関心事のいくつかを示すことができます。\n",
    "\n",
    "## センチメントを決定する\n",
    "\n",
    "センチメントスコア*に基づいて、レビューを*肯定的*または*否定的*に分類すると便利かもしれません。繰り返しになりますが、これを行うにはテキストアナリティクスサービスを使用することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the client and reviews you created previously to get sentiment scores\n",
    "sentiment_analysis = text_analytics_client.sentiment(documents=reviews)\n",
    "\n",
    "# Print the results for each review\n",
    "for review_num in range(len(reviews)):\n",
    "\n",
    "    # Get the sentiment score for this review\n",
    "    sentiment_score = sentiment_analysis.documents[review_num].score\n",
    "\n",
    "    # classifiy 'positive' if more than 0.5, \n",
    "    if sentiment_score < 0.5:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'positive'\n",
    "\n",
    "    # print file name and sentiment\n",
    "    print('{} : {} ({})'.format(reviews[review_num]['id'], sentiment, sentiment_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 既知のエンティティを抽出する\n",
    "\n",
    "*エンティティ*とは、一般的に理解されているタイプのアイテムを参照している文章の中で言及される可能性のあるものです。例えば、場所、人、日付などです。例えば、レビューで言及されているdateaや場所に興味があるとします - それらを見つけるには以下のコードを使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the client and reviews you created previously to get named entities\n",
    "entity_analysis = text_analytics_client.entities(documents=reviews)\n",
    "\n",
    "# Print the results for each review\n",
    "for review_num in range(len(reviews)):\n",
    "    print(reviews[review_num]['id'])\n",
    "    # Get the named entitites in this review\n",
    "    entities = entity_analysis.documents[review_num].entities\n",
    "    for entity in entities:\n",
    "        # Only get location entitites\n",
    "        if entity.type in ['DateTime','Location']:\n",
    "            link = '(' + entity.wikipedia_url + ')' if entity.wikipedia_id is not None else ''\n",
    "            print(' - {}: {} {}'.format(entity.type, entity.name, link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一部のエンティティは、ウィキペディアに関連するページを持っていることが十分に知られていることに注意してください。\n",
    "\n",
    "## Learn More\n",
    "\n",
    "Text Analyticsサービスの詳細については、[Text Analyticsサービスのドキュメント](https://docs.microsoft.com/azure/cognitive-services/text-analytics/)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.6 32-bit",
   "display_name": "Python 3.8.6 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "c617fa10d1ba9d6a36573c3c4f7496e3bb2fbcdd4aeff3a055aadb58d26c8355"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}